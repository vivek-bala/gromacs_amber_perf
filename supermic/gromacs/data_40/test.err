GROMACS:    gmx grompp, VERSION 5.0.7 (double precision)

GROMACS is written by:
Emile Apol         Rossen Apostolov   Herman J.C. Berendsen Par Bjelkmar       
Aldert van Buuren  Rudi van Drunen    Anton Feenstra     Sebastian Fritsch  
Gerrit Groenhof    Christoph Junghans Peter Kasson       Carsten Kutzner    
Per Larsson        Justin A. Lemkul   Magnus Lundborg    Pieter Meulenhoff  
Erik Marklund      Teemu Murtola      Szilard Pall       Sander Pronk       
Roland Schulz      Alexey Shvetsov    Michael Shirts     Alfons Sijbers     
Peter Tieleman     Christian Wennberg Maarten Wolf       
and the project leaders:
Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2014, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx grompp, VERSION 5.0.7 (double precision)
Executable:   /usr/local/packages/gromacs/5.0.7/INTEL-140-OPENMPI-1.8.4/bin/gmx_d
Library dir:  /usr/local/packages/gromacs/5.0.7/INTEL-140-OPENMPI-1.8.4/share/gromacs/top
Command line:
  gmx_d grompp -n index.ndx -f run.mdp -c start.gro -maxwarn 1

Setting the LD random seed to 1547794782
Generated 2278 of the 2278 non-bonded parameter combinations
Generating 1-4 interactions: fudge = 0.5
Generated 2278 of the 2278 1-4 parameter combinations
Excluding 3 bonded neighbours molecule type 'Protein_chain_A'
Excluding 2 bonded neighbours molecule type 'SOL'
Excluding 1 bonded neighbours molecule type 'NA'
Excluding 1 bonded neighbours molecule type 'CL'
Setting gen_seed to 3059988672
Velocities were taken from a Maxwell distribution at 355 K
Removing all charge groups because cutoff-scheme=Verlet
Number of degrees of freedom in T-Coupling group System is 28510.00

NOTE 1 [file run.mdp]:
  You are using geometric combination rules in LJ-PME, but your non-bonded
  C6 parameters do not follow these rules. This will introduce very small
  errors in the forces and energies in your simulations. If your system is
  homogeneous, consider using dispersion correction for the total energy
  and pressure.

Estimate for the relative computational load of the PME mesh part: 0.25

There was 1 note

gcq#88: "Let Me Do This" (Urban Dance Squad)

GROMACS:    gmx mdrun, VERSION 5.0.7 (double precision)

GROMACS is written by:
Emile Apol         Rossen Apostolov   Herman J.C. Berendsen Par Bjelkmar       
Aldert van Buuren  Rudi van Drunen    Anton Feenstra     Sebastian Fritsch  
Gerrit Groenhof    Christoph Junghans Peter Kasson       Carsten Kutzner    
Per Larsson        Justin A. Lemkul   Magnus Lundborg    Pieter Meulenhoff  
Erik Marklund      Teemu Murtola      Szilard Pall       Sander Pronk       
Roland Schulz      Alexey Shvetsov    Michael Shirts     Alfons Sijbers     
Peter Tieleman     Christian Wennberg Maarten Wolf       
and the project leaders:
Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2014, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, VERSION 5.0.7 (double precision)
Executable:   /usr/local/packages/gromacs/5.0.7/INTEL-140-OPENMPI-1.8.4/bin/gmx_mpi_d
Library dir:  /usr/local/packages/gromacs/5.0.7/INTEL-140-OPENMPI-1.8.4/share/gromacs/top
Command line:
  mdrun_mpi_d


Number of hardware threads detected (20) does not match the number reported by OpenMP (1).
Consider setting the launch configuration manually!
Reading file topol.tpr, VERSION 5.0.7 (double precision)

Will use 30 particle-particle and 10 PME only ranks
This is a guess, check the performance at the end of the log file
Using 40 MPI processes
Using 1 OpenMP thread per MPI process

Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

NOTE: Turning on dynamic load balancing


Writing final coordinates.

 Average load imbalance: 4.3 %
 Part of the total run time spent waiting due to load imbalance: 3.5 %
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %
 Average PME mesh/force load: 0.623
 Part of the total run time spent waiting due to PP/PME imbalance: 8.5 %

NOTE: 8.5 % performance was lost because the PME ranks
      had less work to do than the PP ranks.
      You might want to decrease the number of PME ranks
      or decrease the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:     1730.609       43.603     3969.0
                 (ns/day)    (hour/ns)
Performance:       39.634        0.606

gcq#234: "If I Wanted You to Understand This, I Would Explain it Better" (J. Cruijff)

