Unloading compiler-dependent module openmpi_ib/1.8.4
--------------------------------------------------------------------------
PMI2 initialized but returned bad values for size and rank.
This is symptomatic of either a failure to use the
"--mpi=pmi2" flag in SLURM, or a borked PMI2 installation.
If running under SLURM, try adding "-mpi=pmi2" to your
srun command line. If that doesn't work, or if you are
not running under SLURM, try removing or renaming the
pmi2.h header file so PMI2 support will not automatically
be built, reconfigure and build OMPI, and then try again
with only PMI1 support enabled.
--------------------------------------------------------------------------
                  :-) GROMACS - gmx grompp, VERSION 5.1.2 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra   Sebastian Fritsch 
  Gerrit Groenhof   Christoph Junghans   Anca Hamuraru    Vincent Hindriksen
 Dimitrios Karkoulis    Peter Kasson        Jiri Kraus      Carsten Kutzner  
    Per Larsson      Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff 
   Erik Marklund      Teemu Murtola       Szilard Pall       Sander Pronk   
   Roland Schulz     Alexey Shvetsov     Michael Shirts     Alfons Sijbers  
   Peter Tieleman    Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx grompp, VERSION 5.1.2
Executable:   /opt/gromacs/bin/gmx_mpi.real
Data prefix:  /opt/gromacs
Command line:
  gmx_mpi.real grompp -n index.ndx -f run.mdp -c start.gro -maxwarn 1

Setting the LD random seed to 2300954736
Generated 2278 of the 2278 non-bonded parameter combinations
Generating 1-4 interactions: fudge = 0.5
Generated 2278 of the 2278 1-4 parameter combinations
Excluding 3 bonded neighbours molecule type 'Protein_chain_A'
Excluding 2 bonded neighbours molecule type 'SOL'
Excluding 1 bonded neighbours molecule type 'NA'
Excluding 1 bonded neighbours molecule type 'CL'
Setting gen_seed to 2193361978
Velocities were taken from a Maxwell distribution at 355 K
Removing all charge groups because cutoff-scheme=Verlet
Number of degrees of freedom in T-Coupling group System is 28510.00

NOTE 1 [file run.mdp]:
  You are using geometric combination rules in LJ-PME, but your non-bonded
  C6 parameters do not follow these rules. This will introduce very small
  errors in the forces and energies in your simulations. If your system is
  homogeneous, consider using dispersion correction for the total energy
  and pressure.

Estimate for the relative computational load of the PME mesh part: 0.37

There was 1 note

gcq#347: "If There Is No Guitar In The House, You Know It's Owner Can Not Be Trusted" (Gogol Bordello)

turning H bonds into constraints...
turning H bonds into constraints...
turning H bonds into constraints...
turning H bonds into constraints...
Determining Verlet buffer for a tolerance of 0.005 kJ/mol/ps at 355 K
Calculated rlist for 1x1 atom pair-list as 1.114 nm, buffer size 0.114 nm
Set rlist, assuming 4x4 atom pair-list, to 1.041 nm, buffer size 0.041 nm
Note that mdrun will redetermine rlist based on the actual pair-list setup
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 52x52x52, spacing 0.096 0.096 0.096
This run will generate roughly 1 Mb of data
                   :-) GROMACS - gmx mdrun, VERSION 5.1.2 (-:

                            GROMACS is written by:
     Emile Apol      Rossen Apostolov  Herman J.C. Berendsen    Par Bjelkmar   
 Aldert van Buuren   Rudi van Drunen     Anton Feenstra   Sebastian Fritsch 
  Gerrit Groenhof   Christoph Junghans   Anca Hamuraru    Vincent Hindriksen
 Dimitrios Karkoulis    Peter Kasson        Jiri Kraus      Carsten Kutzner  
    Per Larsson      Justin A. Lemkul   Magnus Lundborg   Pieter Meulenhoff 
   Erik Marklund      Teemu Murtola       Szilard Pall       Sander Pronk   
   Roland Schulz     Alexey Shvetsov     Michael Shirts     Alfons Sijbers  
   Peter Tieleman    Teemu Virolainen  Christian Wennberg    Maarten Wolf   
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2015, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, VERSION 5.1.2
Executable:   /opt/gromacs/bin/gmx_mpi.real
Data prefix:  /opt/gromacs
Command line:
  gmx_mpi.real mdrun


Number of logical cores detected (24) does not match the number reported by OpenMP (1).
Consider setting the launch configuration manually!

NOTE: Error occurred during GPU detection:
      no CUDA-capable device is detected
      Can not use GPU acceleration, will fall back to CPU kernels.


Running on 3 nodes with total 72 cores, 72 logical cores, 0 compatible GPUs
  Cores per node:           24
  Logical cores per node:   24
  Compatible GPUs per node:  0
Hardware detected on host comet-22-13.sdsc.edu (the node of MPI rank 0):
  CPU info:
    Vendor: GenuineIntel
    Brand:  Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    SIMD instructions most likely to fit this hardware: AVX2_256
    SIMD instructions selected at GROMACS compile time: AVX2_256

Reading file topol.tpr, VERSION 5.1.2 (single precision)
The number of OpenMP threads was set by environment variable OMP_NUM_THREADS to 1

Will use 45 particle-particle and 27 PME only ranks
This is a guess, check the performance at the end of the log file
Using 72 MPI processes
Using 1 OpenMP thread per MPI process


Non-default thread affinity set probably by the OpenMP library,
disabling internal thread affinity
starting mdrun 'Protein in water'
10000 steps,     20.0 ps.

NOTE: Turning on dynamic load balancing


Writing final coordinates.

 Average load imbalance: 5.0 %
 Part of the total run time spent waiting due to load imbalance: 2.7 %
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %
 Average PME mesh/force load: 0.782
 Part of the total run time spent waiting due to PP/PME imbalance: 6.7 %

NOTE: 6.7 % performance was lost because the PME ranks
      had less work to do than the PP ranks.
      You might want to decrease the number of PME ranks
      or decrease the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:      705.735        9.869     7151.3
                 (ns/day)    (hour/ns)
Performance:      175.117        0.137

gcq#371: "England's dancing days are done" (P. J. Harvey)

